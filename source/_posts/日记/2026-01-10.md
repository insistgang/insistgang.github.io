---
title: 2026年1月10日
date: 2026-01-10 00:00:00
tags:
  - 日记
cover: /img/riji.png
abbrlink: 0110h5i6
---

## 昨日计划完成情况

- [x] 专注搞小程序 —— 完成，下午开发+晚上修bug
- [x] 看论文 Day09 —— 完成，Swin Transformer

## 今日记录

**上午**

- 睡到10点多，打了两把王者，周六放松一下
- 和景哥、涂涂去遇上西雅图吃火锅烤肉自助
- 顺便给电车充电，检查三创赛审核——实战赛和常规赛都通过了✅

**下午 @ 实验室**

- 研究教师资格证报名，纠结报中职还是高中数学（后来发现大学老师需要的是高校教资，另一套体系）
- 用Gemini把蓝底照片换成白底
- 16:00开始搞小程序开发

**晚上**

- 和仕缘吃麻辣烫
- 回来继续修小程序bug
- 20:50-21:30散步，期间看论文
- 完成Day09论文，补了不少基础知识

## 状态

精力 7/10，情绪 5/10

周六节奏比较健康，休息、社交、学习都有。但情绪一般。

## 健康打卡

- 午餐：遇上西雅图火锅烤肉自助（81元）
- 晚餐：麻辣烫（27元）
- 散步：✅ 40分钟
- 喝水：✅ 2000ml

## 今日收获

- 三创赛实战赛+常规赛审核通过
- 小程序开发推进+bug修复
- Swin Transformer论文读完，理解了Shifted Window Attention的精髓

## 今日卡点

- 论文基础知识欠缺，需要花大量时间追根溯源补课

## 感悟

> 失败是常态，要勇敢战斗，永不言弃。

## 明日计划

- [ ] 完成小程序
- [ ] 本周复盘
- [ ] 整理工作内容，想清楚怎么推进
- [ ] 归纳整理自己的东西
- [ ] 好好休息

---

## 今日论文 #day09

**Swin Transformer《Swin Transformer: Hierarchical Vision Transformer using Shifted Windows》**
ICCV 2021 | 微软亚研 | Ze Liu等

### 核心痛点

ViT的O(n²)复杂度限制了高分辨率图像处理能力。分类用224×224还行，检测/分割需要高分辨率就吃不消。非分层结构也难构建多尺度特征。

### 解法

**Shifted Window Attention**：

- 图像切分成不重叠的window（7×7），window内做self-attention，复杂度O(n²)→O(n)
- 下一层shift window（移动半个window），让跨window信息流动
- 用masked attention处理边界问题
- 层级下采样产生特征金字塔：4个stage，分辨率逐层减半，通道数逐层翻倍

### 关键发现

- ImageNet-1K：Swin-L 87.3% top-1，超越ViT和ResNeSt
- COCO检测：Swin-L 58.7 box AP，51.1 mask AP
- ADE20K分割：53.5 mIoU，SOTA

### 读后感

Day06 ViT说"CNN的归纳偏置是枷锁"，今天Swin说"局部性其实是个好东西"。Swin把attention限制在window内，通过shift实现全局连接——本质上融合了CNN的局部性和Transformer的全局建模能力。Transformer不是要完全取代CNN，而是要学习CNN的优点。
